extends layout

block title     
  title Background Aware Movie Conversations Dataset
  
block description
  meta(name='description', content='Holl-E is a domain specific background aware conversations dataset')

block extralinks
  link(rel='stylesheet', href='https://github.com/nikitacs16/holl-e-website/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin trained_on_random_negatives(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th ROUGE-L
      th F1
      tr
        td
          p #{"1"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER-Large 
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al., '17)
        td #{68.92}
        td #{0.42}

      tr
        td
          p #{"2"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | DEB <b>
          p.institution #{"IIT Madras"}
          a(href="") (Sai & al. '20)
        td #{66.78}
        td #{0.39}

      tr
        td
          p #{"3"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER <b>
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al., '17)
        td #{65.00}
        td #{0.35}

      tr
        td
          p #{"4"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | ADEM <b>
          p.institution #{"McGill University"}
          a(href="https://arxiv.org/abs/1708.07149") (Lowe & al., '17)
        td #{64.43}
        td #{0.37}

      tr
        td
          p #{"5"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | BERT+DNN <b>
          p.institution #{"University of Southern California"}
          a(href="https://arxiv.org/abs/1904.10635") (Ghazarian et al., 2019)
        td #{60.14}
        td #{0.29}


mixin trained_on_mixed_long(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th Accuracy
      th PBC       
      tr
        td
          p #{"1"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | DEB <b>
          p.institution #{"IIT Madras"}
          a(href="") (Sai & al. '20)
        td #{92.66}
        td #{0.86}

      tr
        td
          p #{"2"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | BERT+DNN <b>
          p.institution #{"University of Southern California"}
          a(href="https://arxiv.org/abs/1904.10635") (Ghazarian et al., 2019)
        td #{86.61}
        td #{0.79}

      tr
        td
          p #{"3"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER-Large <b>
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al. '17)
        td #{86.52}
        td #{0.78}

      tr
        td
          p #{"4"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER <b>
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al. '17)
        td #{83.81}
        td #{0.74}

      tr
        td
          p #{"5"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | ADEM <b>
          p.institution #{"McGill University"}
          a(href="https://arxiv.org/abs/1708.07149") (Lowe & al., '17)
        td #{66.62}
        td #{0.41}

mixin trained_on_mixed_short(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th Accuracy
      th PBC       
      tr
        td
          p #{"1"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | DEB <b>
          p.institution #{"IIT Madras"}
          a(href="") (Sai & al. '20)
        td #{92.66}
        td #{0.86}

      tr
        td
          p #{"2"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | BERT+DNN <b>
          p.institution #{"University of Southern California"}
          a(href="https://arxiv.org/abs/1904.10635") (Ghazarian et al., 2019)
        td #{86.61}
        td #{0.79}

      tr
        td
          p #{"3"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER-Large <b>
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al. '17)
        td #{86.52}
        td #{0.78}

      tr
        td
          p #{"4"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | RUBER <b>
          p.institution #{"Peking University"}
          a(href="https://arxiv.org/abs/1701.03079") (Tao & al. '17)
        td #{83.81}
        td #{0.74}

      tr
        td
          p #{"5"}
          span.date.label.label-default #{"Sep 13, 2020"}
        td
          | ADEM <b>
          p.institution #{"McGill University"}
          a(href="https://arxiv.org/abs/1708.07149") (Lowe & al., '17)
        td #{66.62}
        td #{0.41}



block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is Holl-E?
              p 
                span 
                | Holl-E is a background aware movie conversations dataset consisting of ~9K chats with at least three background resources associated with the chat. Every alternate utterance is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie.              
                a.btn.actionBtn(href="https://www.aclweb.org/anthology/D18-1255/") Holl-E paper (EMNLP 2018)
              
              hr
              .infoHeadline
                h2 Getting Started
              p 
                | Download the dataset of 9k chats with at least three background resources per chat. 
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://drive.google.com/open?id=1xQBRDs5q_2xLOdOpbq7UeAmUM0Ht370A", download)
                      | Download dataset
              hr
              .infoHeadline
                h2 Models
                p 
                span
                  | Our baselines consist of 1. pure generation based models which ignore the background knowledge 2. generation based models which learn to copy information from the background knowledge when required and 3. span prediction based models which predict the appropriate response span in the background knowledge. We also have different splits of the dataset to test different capabilities of new architectures. We also provide a multi-reference evaluation for our test set. Please see the code repository for more details.           
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://github.com/nikitacs16/Holl-E", download)
                      | Code
                  
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at   
                a(href="mailto:nikitamoghe29@gmail.com") nikitamoghe29@gmail.com
                | .
            .infoSubheadline
              include includes/tweet
              include includes/github
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Results (trained on oracle)
              p 
                | Holl-E evaluates the ability of new architectures to produce responses grounded in knowledge. For a given context of the conversation, the task is to generate responses by copying and/or modifying contiguous segments from the background knowledge. The resources consist of the plot of the movie, review of the movie, some comments about the movie, or a fact table. We provide both the span-level annotation and the actual utterance for every alternate response. Thus, new architectures can experiment with both   e, we report the test perfromance of various models on different datasplits,  We use ROUGE-L and BLEU-4 as the performance measure. The other measures and human evaluation are discussed in the paper. We report the performance of different models 
              +trained_on_random_negatives(test2, true)
          .infoCard
            .infoBody
              .infoHeadline
                h2 Results (trained on mixed-long)
              p 
                |  Here, we report the test perfromance of various models on the mixed-long version of the dataset. In this setup, all the background resources are provided as they are with the chat. This setup measures the ability of the system to retrieve the correct background resource first and then generate a response 
              +trained_on_mixed_long(test1, true)
          .infoCard
            .infoBody
              .infoHeadline              
                h2 Results (trained on mixed-short)
              p 
                |  Here, we report the test perfromance of various models on the mixed-short version of the dataset. In this setup, 
              +trained_on_mixed_short(test1, true)

